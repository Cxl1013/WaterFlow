{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入相关包\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据读入，并将初赛决赛相同数据合并concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 历史入库数据\n",
    "ruku_data = pd.read_excel('./初赛/入库流量数据.xlsx')\n",
    "ruku_data_jue = pd.read_excel('./决赛/入库流量数据.xlsx')\n",
    "# 数据合并\n",
    "ruku_data = pd.concat([ruku_data,ruku_data_jue],axis=0)\n",
    "#时间类型转换\n",
    "ruku_data['TimeStample_cov'] = pd.to_datetime(ruku_data['TimeStample'])\n",
    "\n",
    "# 环境数据，温度，风速，同上\n",
    "huanjing_data = pd.read_excel('./初赛/环境表.xlsx')\n",
    "huanjing_data_jue = pd.read_excel('./决赛/环境表.xlsx')\n",
    "huanjing_data = pd.concat([huanjing_data,huanjing_data_jue],axis=0)\n",
    "huanjing_data.columns = ['TimeStample','气温','风速','风向']\n",
    "huanjing_data['daysd'] = pd.to_datetime(huanjing_data['TimeStample'])\n",
    "\n",
    "# 未来5天天气预报,降水量\n",
    "tianqiyubao_data = pd.read_excel('./初赛/降雨预报数据.xlsx')\n",
    "tianqiyubao_data_jue = pd.read_excel('./决赛/降雨预报数据.xlsx')\n",
    "tianqiyubao_data = pd.concat([tianqiyubao_data,tianqiyubao_data_jue],axis=0)\n",
    "tianqiyubao_data['TimeStample_cov'] = pd.to_datetime(tianqiyubao_data['TimeStample'])\n",
    "\n",
    "\n",
    "#遥测站数据\n",
    "yaocezhan_data = pd.read_excel('./初赛/遥测站降雨数据.xlsx')\n",
    "yaocezhan_data_jue = pd.read_excel('./决赛/遥测站降雨数据.xlsx')\n",
    "yaocezhan_data = pd.concat([yaocezhan_data,yaocezhan_data_jue],axis=0)\n",
    "yaocezhan_data['TimeStample_cov'] = pd.to_datetime(yaocezhan_data['TimeStample'])\n",
    "# 求得当天遥测站的sum\n",
    "yaocezhan_data['all_rain'] = yaocezhan_data[['R'+str(i+1) for i in range(39)]].sum(axis=1)\n",
    "yaocezhan_data['mean_rain'] = yaocezhan_data[['R'+str(i+1) for i in range(39)]].sum(axis=1)\n",
    "yaocezhan_data.set_index('TimeStample_cov', inplace=True)\n",
    "# 将遥测站数据转换成3小时的时间段，方便数据进行关联\n",
    "yaocezhan_data_resample = yaocezhan_data.resample('3H').sum().reset_index()\n",
    "yaocezhan_data_resample['TimeStample_cov'] = yaocezhan_data_resample['TimeStample_cov']+datetime.timedelta(hours=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征工程，测试集、训练集构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决赛预测时段\n",
    "# 1.【2019年2月1日至2月7日】\n",
    "# 2.【2019年4月1日至4月7日】\n",
    "# 3.【2019年6月1日至6月7日】\n",
    "# 4.【2019年8月1日至8月7日】\n",
    "# 5.【2019年11月1日至11月7日】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集构造\n",
    "create_df = ruku_data.copy()\n",
    "create_df = create_df.merge(yaocezhan_data_resample[['TimeStample_cov','all_rain','mean_rain']],how='left',on='TimeStample_cov')\n",
    "# create_df.Qi.fillna(0,inplace=True)\n",
    "\n",
    "#创建分段一测试数据集\n",
    "cut_1 = pd.DataFrame()\n",
    "cut_1['TimeStample_cov'] = pd.date_range('2019-02-01 02:00:00', '2019-02-08', freq='3h')\n",
    "cut_1 = cut_1.merge(yaocezhan_data_resample[['TimeStample_cov','all_rain','mean_rain']],how='left',on='TimeStample_cov')\n",
    "\n",
    "#创建分段二测试数据集\n",
    "cut_2 = pd.DataFrame()\n",
    "cut_2['TimeStample_cov'] = pd.date_range('2019-04-01 02:00:00', '2019-04-08', freq='3h')\n",
    "cut_2 = cut_2.merge(yaocezhan_data_resample[['TimeStample_cov','all_rain','mean_rain']],how='left',on='TimeStample_cov')\n",
    "\n",
    "#创建分段三测试数据集\n",
    "cut_3 = pd.DataFrame()\n",
    "cut_3['TimeStample_cov'] = pd.date_range('2019-06-01 02:00:00', '2019-06-08', freq='3h')\n",
    "cut_3 = cut_3.merge(yaocezhan_data_resample[['TimeStample_cov','all_rain','mean_rain']],how='left',on='TimeStample_cov')\n",
    "cut_3 = cut_3.fillna(0)\n",
    "\n",
    "#创建分段四测试数据集\n",
    "cut_4 = pd.DataFrame()\n",
    "cut_4['TimeStample_cov'] = pd.date_range('2019-08-01 02:00:00', '2019-08-08', freq='3h')\n",
    "cut_4 = cut_4.merge(yaocezhan_data_resample[['TimeStample_cov','all_rain','mean_rain']],how='left',on='TimeStample_cov')\n",
    "cut_4 = cut_4.fillna(0)\n",
    "\n",
    "#创建分段五 测试数据集\n",
    "cut_5 = pd.DataFrame()\n",
    "cut_5['TimeStample_cov'] = pd.date_range('2019-11-01 02:00:00', '2019-11-08', freq='3h')\n",
    "cut_5 = cut_5.merge(yaocezhan_data_resample[['TimeStample_cov','all_rain','mean_rain']],how='left',on='TimeStample_cov')\n",
    "cut_5 = cut_5.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据时间转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换时间\n",
    "create_df['daysd'] = pd.to_datetime(create_df.TimeStample_cov.map(lambda x:x.strftime('%Y-%m-%d')))\n",
    "cut_2['daysd'] = pd.to_datetime(cut_2.TimeStample_cov.map(lambda x:x.strftime('%Y-%m-%d')))\n",
    "cut_1['daysd'] = pd.to_datetime(cut_1.TimeStample_cov.map(lambda x:x.strftime('%Y-%m-%d')))\n",
    "cut_3['daysd'] = pd.to_datetime(cut_3.TimeStample_cov.map(lambda x:x.strftime('%Y-%m-%d')))\n",
    "cut_4['daysd'] = pd.to_datetime(cut_4.TimeStample_cov.map(lambda x:x.strftime('%Y-%m-%d')))\n",
    "cut_5['daysd'] = pd.to_datetime(cut_5.TimeStample_cov.map(lambda x:x.strftime('%Y-%m-%d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前3天天气预报进行关联，只是用前3天。\n",
    "tianqiyubao_data = pd.read_excel('./初赛/降雨预报数据.xlsx')\n",
    "tianqiyubao_data['TimeStample'] = pd.to_datetime(tianqiyubao_data['TimeStample'])\n",
    "tianqiyubao_data.columns = ['daysd','D1','D2','D3','D4','D5']\n",
    "tianqiyubao_data['daysd'] = tianqiyubao_data['daysd']+datetime.timedelta(days=3)\n",
    "create_df = create_df.merge(tianqiyubao_data[['daysd','D1','D2','D3']],how='left',on='daysd')\n",
    "\n",
    "cut_1 = cut_1.merge(tianqiyubao_data[['daysd','D1','D2','D3']],how='left',on='daysd')\n",
    "cut_2 = cut_2.merge(tianqiyubao_data[['daysd','D1','D2','D3']],how='left',on='daysd')\n",
    "cut_3 = cut_3.merge(tianqiyubao_data[['daysd','D1','D2','D3']],how='left',on='daysd')\n",
    "cut_4 = cut_4.merge(tianqiyubao_data[['daysd','D1','D2','D3']],how='left',on='daysd')\n",
    "cut_5 = cut_5.merge(tianqiyubao_data[['daysd','D1','D2','D3']],how='left',on='daysd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关联环境因素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试机训练集，关联环境因素\n",
    "create_df = create_df.merge(huanjing_data[['daysd','气温','风速']],how='left',on='daysd')\n",
    "cut_1 = cut_1.merge(huanjing_data[['daysd','气温','风速']],how='left',on='daysd')\n",
    "\n",
    "cut_2 = cut_2.merge(huanjing_data[['daysd','气温','风速']],how='left',on='daysd')\n",
    "\n",
    "cut_3 = cut_3.merge(huanjing_data[['daysd','气温','风速']],how='left',on='daysd')\n",
    "\n",
    "cut_4 = cut_4.merge(huanjing_data[['daysd','气温','风速']],how='left',on='daysd')\n",
    "\n",
    "cut_5 = cut_5.merge(huanjing_data[['daysd','气温','风速']],how='left',on='daysd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关联前8个时间段数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_df['Qi_1'] = create_df.Qi.shift(1)\n",
    "create_df['Qi_2'] = create_df.Qi.shift(2)\n",
    "create_df['Qi_3'] = create_df.Qi.shift(3)\n",
    "create_df['Qi_4'] = create_df.Qi.shift(4)\n",
    "create_df['Qi_5'] = create_df.Qi.shift(5)\n",
    "create_df['Qi_6'] = create_df.Qi.shift(6)\n",
    "create_df['Qi_7'] = create_df.Qi.shift(7)\n",
    "create_df['Qi_8'] = create_df.Qi.shift(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动填写各个预测分段前面8个时间点的数据，比如2月1号，就填写1月31号的后8个时间点数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以使用该语句进行查看前8个时间点\n",
    "# create_df[create_df.TimeStample>='2019-10-31'][:8].Qi\n",
    "# 填写时间段1\n",
    "cut_1['Qi_1'] = 0.081589\n",
    "cut_1['Qi_2'] = 0.049254\n",
    "cut_1['Qi_3'] = 0.021640\n",
    "cut_1['Qi_4'] = 0.005774\n",
    "cut_1['Qi_5'] = 0.191796\n",
    "cut_1['Qi_6'] = 0.171462\n",
    "cut_1['Qi_7'] = 0.133404\n",
    "cut_1['Qi_8'] = 0.141035\n",
    "# 时间段2\n",
    "cut_2['Qi_1'] = 0.238289\n",
    "cut_2['Qi_2'] = 0.153286\n",
    "cut_2['Qi_3'] = 0.103078\n",
    "cut_2['Qi_4'] = 0.088517\n",
    "cut_2['Qi_5'] = 0.244264\n",
    "cut_2['Qi_6'] = 0.289050\n",
    "cut_2['Qi_7'] = 0.356580\n",
    "cut_2['Qi_8'] = 0.346287\n",
    "\n",
    "cut_3['Qi_1'] = 0.179043\n",
    "cut_3['Qi_2'] = 0.027514\n",
    "cut_3['Qi_3'] = 0.043480\n",
    "cut_3['Qi_4'] = 0.051162\n",
    "cut_3['Qi_5'] = 0.050158\n",
    "cut_3['Qi_6'] = 0.118391\n",
    "cut_3['Qi_7'] = 0.107245\n",
    "cut_3['Qi_8'] = 0.045338\n",
    "\n",
    "cut_4['Qi_1'] = 0.042526\n",
    "cut_4['Qi_2'] = 0.014611\n",
    "cut_4['Qi_3'] = 0.014611\n",
    "cut_4['Qi_4'] = 0.015012\n",
    "cut_4['Qi_5'] = 0.033188\n",
    "cut_4['Qi_6'] = 0.028518\n",
    "cut_4['Qi_7'] = 0.028518\n",
    "cut_4['Qi_8'] = 0.075011\n",
    "\n",
    "cut_5['Qi_1'] = 0.188281\n",
    "cut_5['Qi_2'] = 0.411458\n",
    "cut_5['Qi_3'] = 0.293920\n",
    "cut_5['Qi_4'] = 0.079681\n",
    "cut_5['Qi_5'] = 0.214942\n",
    "cut_5['Qi_6'] = 0.077723\n",
    "cut_5['Qi_7'] = 0.294472\n",
    "cut_5['Qi_8'] = 0.193403"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 年月日特征、以及数据排序特征，保留数据顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年、月、日、小时排序特征\n",
    "create_df['year'] = create_df.TimeStample_cov.dt.year\n",
    "create_df['month'] = create_df.TimeStample_cov.dt.month\n",
    "create_df['hour'] = create_df.TimeStample_cov.dt.hour\n",
    "create_df['hourIndex'] = create_df.apply(lambda x:time.mktime(x['TimeStample_cov'].timetuple()),axis=1 )\n",
    "create_df['hourIndex'] = create_df.apply(lambda x: int(x['hourIndex'] /(3600)) ,axis=1 ) \n",
    "\n",
    "#各个分段同上\n",
    "cut_1['year'] = cut_1.TimeStample_cov.dt.year\n",
    "cut_1['month'] = cut_1.TimeStample_cov.dt.month\n",
    "cut_1['hour'] = cut_1.TimeStample_cov.dt.hour\n",
    "cut_1['hourIndex'] = cut_1.apply(lambda x:time.mktime(x['TimeStample_cov'].timetuple()),axis=1 )\n",
    "cut_1['hourIndex'] = cut_1.apply(lambda x: int(x['hourIndex'] /(3600)) ,axis=1 ) \n",
    "\n",
    "cut_2['year'] = cut_2.TimeStample_cov.dt.year\n",
    "cut_2['month'] = cut_2.TimeStample_cov.dt.month\n",
    "cut_2['hour'] = cut_2.TimeStample_cov.dt.hour\n",
    "cut_2['hourIndex'] = cut_2.apply(lambda x:time.mktime(x['TimeStample_cov'].timetuple()),axis=1 )\n",
    "cut_2['hourIndex'] = cut_2.apply(lambda x: int(x['hourIndex'] /(3600)) ,axis=1 ) \n",
    "\n",
    "cut_3['year'] = cut_3.TimeStample_cov.dt.year\n",
    "cut_3['month'] = cut_3.TimeStample_cov.dt.month\n",
    "cut_3['hour'] = cut_3.TimeStample_cov.dt.hour\n",
    "cut_3['hourIndex'] = cut_3.apply(lambda x:time.mktime(x['TimeStample_cov'].timetuple()),axis=1 )\n",
    "cut_3['hourIndex'] = cut_3.apply(lambda x: int(x['hourIndex'] /(3600)) ,axis=1 )\n",
    "\n",
    "cut_4['year'] = cut_4.TimeStample_cov.dt.year\n",
    "cut_4['month'] = cut_4.TimeStample_cov.dt.month\n",
    "cut_4['hour'] = cut_4.TimeStample_cov.dt.hour\n",
    "cut_4['hourIndex'] = cut_4.apply(lambda x:time.mktime(x['TimeStample_cov'].timetuple()),axis=1 )\n",
    "cut_4['hourIndex'] = cut_4.apply(lambda x: int(x['hourIndex'] /(3600)) ,axis=1 )\n",
    "\n",
    "cut_5['year'] = cut_5.TimeStample_cov.dt.year\n",
    "cut_5['month'] = cut_5.TimeStample_cov.dt.month\n",
    "cut_5['hour'] = cut_5.TimeStample_cov.dt.hour\n",
    "cut_5['hourIndex'] = cut_5.apply(lambda x:time.mktime(x['TimeStample_cov'].timetuple()),axis=1 )\n",
    "cut_5['hourIndex'] = cut_5.apply(lambda x: int(x['hourIndex'] /(3600)) ,axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集使用的特征筛选\n",
    "usecol = [i for i in create_df.columns if i not in['TimeStample','TimeStample_cov','daysd']]\n",
    "end_usecol = [i for i in create_df.columns if i not in['TimeStample','TimeStample_cov','daysd','Qi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要使用的数据\n",
    "usedata = create_df[usecol]\n",
    "usedata = usedata.dropna()\n",
    "usedata.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建一个lightgbm方法模型，采用五折交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lgb_model(train_data,y,N):\n",
    "    # 超参数\n",
    "    lgb_param_l1 = { 'learning_rate': 0.01, \n",
    "                    'boosting_type': 'gbdt', \n",
    "                    'objective': 'regression_l1',\n",
    "                    'metric': 'mae',\n",
    "                    'min_child_samples': 46, \n",
    "                    'min_child_weight': 0.01,\n",
    "                    'feature_fraction': 0.8, \n",
    "                    'bagging_fraction': 0.8, \n",
    "                    'bagging_freq': 2, \n",
    "                    'num_leaves': 16, \n",
    "                    'max_depth': 5, \n",
    "                    'n_jobs': -1, \n",
    "                    'seed': 2019, }\n",
    "    # 交叉验证\n",
    "    skf=KFold(n_splits=N,shuffle=True,random_state=42)\n",
    "    oof_lgb=np.zeros(train_data.shape[0])\n",
    "    models = []\n",
    "    for i,(tr,va) in enumerate(skf.split(train_data,y)):\n",
    "        print('fold:',i+1,'training')\n",
    "        X_train, X_test, y_train, y_test = train_data[tr],train_data[va],y[tr],y[va]\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_valid = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "        clf = lgb.train(lgb_param_l1,lgb_train, \n",
    "                         valid_sets=[lgb_valid],#, xgtrain], \n",
    "                         valid_names=['valid'],#,'train'], \n",
    "                         num_boost_round=30000,\n",
    "                         early_stopping_rounds=100,\n",
    "                         verbose_eval=50)\n",
    "        oof_lgb[va] += clf.predict(X_test,num_iteration=clf.best_iteration)\n",
    "        models.append(clf)\n",
    "    print(\"stacking的score: {:<8.8f}\".format(np.sqrt(mean_squared_error(oof_lgb, y))))\n",
    "    return oof_lgb,models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1 training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's l1: 0.108438\n",
      "[100]\tvalid's l1: 0.0824995\n",
      "[150]\tvalid's l1: 0.068932\n",
      "[200]\tvalid's l1: 0.0617115\n",
      "[250]\tvalid's l1: 0.0577688\n",
      "[300]\tvalid's l1: 0.0552005\n",
      "[350]\tvalid's l1: 0.0540969\n",
      "[400]\tvalid's l1: 0.0534904\n",
      "[450]\tvalid's l1: 0.0530915\n",
      "[500]\tvalid's l1: 0.0528377\n",
      "[550]\tvalid's l1: 0.0526683\n",
      "[600]\tvalid's l1: 0.0525351\n",
      "[650]\tvalid's l1: 0.0524665\n",
      "[700]\tvalid's l1: 0.052414\n",
      "[750]\tvalid's l1: 0.0523598\n",
      "[800]\tvalid's l1: 0.0523144\n",
      "[850]\tvalid's l1: 0.0522837\n",
      "[900]\tvalid's l1: 0.0522497\n",
      "[950]\tvalid's l1: 0.052223\n",
      "[1000]\tvalid's l1: 0.0521967\n",
      "[1050]\tvalid's l1: 0.0521812\n",
      "[1100]\tvalid's l1: 0.052163\n",
      "[1150]\tvalid's l1: 0.0521579\n",
      "[1200]\tvalid's l1: 0.0521395\n",
      "[1250]\tvalid's l1: 0.0521228\n",
      "[1300]\tvalid's l1: 0.0521214\n",
      "[1350]\tvalid's l1: 0.0521075\n",
      "[1400]\tvalid's l1: 0.0520992\n",
      "[1450]\tvalid's l1: 0.0520903\n",
      "[1500]\tvalid's l1: 0.0520873\n",
      "[1550]\tvalid's l1: 0.052086\n",
      "[1600]\tvalid's l1: 0.0520741\n",
      "[1650]\tvalid's l1: 0.0520706\n",
      "[1700]\tvalid's l1: 0.0520717\n",
      "Early stopping, best iteration is:\n",
      "[1626]\tvalid's l1: 0.0520671\n",
      "fold: 2 training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's l1: 0.104295\n",
      "[100]\tvalid's l1: 0.0786063\n",
      "[150]\tvalid's l1: 0.0657171\n",
      "[200]\tvalid's l1: 0.0588149\n",
      "[250]\tvalid's l1: 0.0551709\n",
      "[300]\tvalid's l1: 0.0530035\n",
      "[350]\tvalid's l1: 0.0520003\n",
      "[400]\tvalid's l1: 0.051435\n",
      "[450]\tvalid's l1: 0.0510783\n",
      "[500]\tvalid's l1: 0.0508553\n",
      "[550]\tvalid's l1: 0.0506309\n",
      "[600]\tvalid's l1: 0.0505121\n",
      "[650]\tvalid's l1: 0.0504254\n",
      "[700]\tvalid's l1: 0.0503598\n",
      "[750]\tvalid's l1: 0.0503123\n",
      "[800]\tvalid's l1: 0.0502581\n",
      "[850]\tvalid's l1: 0.0502299\n",
      "[900]\tvalid's l1: 0.0502064\n",
      "[950]\tvalid's l1: 0.0501992\n",
      "[1000]\tvalid's l1: 0.0501699\n",
      "[1050]\tvalid's l1: 0.0501474\n",
      "[1100]\tvalid's l1: 0.0501268\n",
      "[1150]\tvalid's l1: 0.0500976\n",
      "[1200]\tvalid's l1: 0.0500804\n",
      "[1250]\tvalid's l1: 0.0500595\n",
      "[1300]\tvalid's l1: 0.0500309\n",
      "[1350]\tvalid's l1: 0.0500199\n",
      "[1400]\tvalid's l1: 0.049998\n",
      "[1450]\tvalid's l1: 0.0499763\n",
      "[1500]\tvalid's l1: 0.0499723\n",
      "[1550]\tvalid's l1: 0.0499645\n",
      "[1600]\tvalid's l1: 0.0499528\n",
      "[1650]\tvalid's l1: 0.049937\n",
      "[1700]\tvalid's l1: 0.0499234\n",
      "[1750]\tvalid's l1: 0.0498984\n",
      "[1800]\tvalid's l1: 0.0498882\n",
      "[1850]\tvalid's l1: 0.0498778\n",
      "[1900]\tvalid's l1: 0.0498619\n",
      "[1950]\tvalid's l1: 0.0498536\n",
      "[2000]\tvalid's l1: 0.0498368\n",
      "[2050]\tvalid's l1: 0.0498267\n",
      "[2100]\tvalid's l1: 0.0498249\n",
      "[2150]\tvalid's l1: 0.0498075\n",
      "[2200]\tvalid's l1: 0.0498017\n",
      "[2250]\tvalid's l1: 0.0497923\n",
      "[2300]\tvalid's l1: 0.0497925\n",
      "[2350]\tvalid's l1: 0.0497782\n",
      "[2400]\tvalid's l1: 0.049765\n",
      "[2450]\tvalid's l1: 0.0497609\n",
      "[2500]\tvalid's l1: 0.0497522\n",
      "[2550]\tvalid's l1: 0.0497533\n",
      "[2600]\tvalid's l1: 0.0497514\n",
      "Early stopping, best iteration is:\n",
      "[2525]\tvalid's l1: 0.0497479\n",
      "fold: 3 training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's l1: 0.105217\n",
      "[100]\tvalid's l1: 0.0790171\n",
      "[150]\tvalid's l1: 0.0657738\n",
      "[200]\tvalid's l1: 0.0587006\n",
      "[250]\tvalid's l1: 0.0547931\n",
      "[300]\tvalid's l1: 0.0527381\n",
      "[350]\tvalid's l1: 0.051813\n",
      "[400]\tvalid's l1: 0.0512726\n",
      "[450]\tvalid's l1: 0.0509714\n",
      "[500]\tvalid's l1: 0.0507782\n",
      "[550]\tvalid's l1: 0.0506893\n",
      "[600]\tvalid's l1: 0.0506106\n",
      "[650]\tvalid's l1: 0.050571\n",
      "[700]\tvalid's l1: 0.0505474\n",
      "[750]\tvalid's l1: 0.0505258\n",
      "[800]\tvalid's l1: 0.050514\n",
      "[850]\tvalid's l1: 0.0504959\n",
      "[900]\tvalid's l1: 0.0504867\n",
      "[950]\tvalid's l1: 0.0504701\n",
      "[1000]\tvalid's l1: 0.0504646\n",
      "[1050]\tvalid's l1: 0.0504459\n",
      "[1100]\tvalid's l1: 0.0504385\n",
      "[1150]\tvalid's l1: 0.0504255\n",
      "[1200]\tvalid's l1: 0.0504186\n",
      "[1250]\tvalid's l1: 0.0504098\n",
      "[1300]\tvalid's l1: 0.0504003\n",
      "[1350]\tvalid's l1: 0.0503975\n",
      "[1400]\tvalid's l1: 0.0503932\n",
      "[1450]\tvalid's l1: 0.0503867\n",
      "[1500]\tvalid's l1: 0.0503737\n",
      "[1550]\tvalid's l1: 0.0503555\n",
      "[1600]\tvalid's l1: 0.0503597\n",
      "[1650]\tvalid's l1: 0.0503578\n",
      "Early stopping, best iteration is:\n",
      "[1586]\tvalid's l1: 0.0503523\n",
      "fold: 4 training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's l1: 0.104313\n",
      "[100]\tvalid's l1: 0.0795087\n",
      "[150]\tvalid's l1: 0.0670026\n",
      "[200]\tvalid's l1: 0.060345\n",
      "[250]\tvalid's l1: 0.0566138\n",
      "[300]\tvalid's l1: 0.0545096\n",
      "[350]\tvalid's l1: 0.0535587\n",
      "[400]\tvalid's l1: 0.0530688\n",
      "[450]\tvalid's l1: 0.0527538\n",
      "[500]\tvalid's l1: 0.0525755\n",
      "[550]\tvalid's l1: 0.0524601\n",
      "[600]\tvalid's l1: 0.0524028\n",
      "[650]\tvalid's l1: 0.0523767\n",
      "[700]\tvalid's l1: 0.0523494\n",
      "[750]\tvalid's l1: 0.0523301\n",
      "[800]\tvalid's l1: 0.0523222\n",
      "[850]\tvalid's l1: 0.0523216\n",
      "[900]\tvalid's l1: 0.0523165\n",
      "[950]\tvalid's l1: 0.0523063\n",
      "[1000]\tvalid's l1: 0.052301\n",
      "[1050]\tvalid's l1: 0.0522916\n",
      "[1100]\tvalid's l1: 0.0522667\n",
      "[1150]\tvalid's l1: 0.0522539\n",
      "[1200]\tvalid's l1: 0.0522305\n",
      "[1250]\tvalid's l1: 0.0522119\n",
      "[1300]\tvalid's l1: 0.0521911\n",
      "[1350]\tvalid's l1: 0.052193\n",
      "[1400]\tvalid's l1: 0.0521925\n",
      "[1450]\tvalid's l1: 0.0521908\n",
      "[1500]\tvalid's l1: 0.0521793\n",
      "[1550]\tvalid's l1: 0.0521773\n",
      "[1600]\tvalid's l1: 0.0521704\n",
      "[1650]\tvalid's l1: 0.0521557\n",
      "[1700]\tvalid's l1: 0.0521592\n",
      "[1750]\tvalid's l1: 0.0521445\n",
      "[1800]\tvalid's l1: 0.052126\n",
      "[1850]\tvalid's l1: 0.0521178\n",
      "[1900]\tvalid's l1: 0.0521138\n",
      "[1950]\tvalid's l1: 0.0521139\n",
      "Early stopping, best iteration is:\n",
      "[1892]\tvalid's l1: 0.0521105\n",
      "fold: 5 training\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid's l1: 0.101871\n",
      "[100]\tvalid's l1: 0.0780869\n",
      "[150]\tvalid's l1: 0.0663914\n",
      "[200]\tvalid's l1: 0.0602672\n",
      "[250]\tvalid's l1: 0.0568113\n",
      "[300]\tvalid's l1: 0.0549961\n",
      "[350]\tvalid's l1: 0.0540574\n",
      "[400]\tvalid's l1: 0.0534944\n",
      "[450]\tvalid's l1: 0.0531573\n",
      "[500]\tvalid's l1: 0.0529561\n",
      "[550]\tvalid's l1: 0.0528081\n",
      "[600]\tvalid's l1: 0.0527292\n",
      "[650]\tvalid's l1: 0.0526714\n",
      "[700]\tvalid's l1: 0.0526278\n",
      "[750]\tvalid's l1: 0.0525897\n",
      "[800]\tvalid's l1: 0.0525405\n",
      "[850]\tvalid's l1: 0.0525081\n",
      "[900]\tvalid's l1: 0.0524725\n",
      "[950]\tvalid's l1: 0.0524429\n",
      "[1000]\tvalid's l1: 0.0524117\n",
      "[1050]\tvalid's l1: 0.0523755\n",
      "[1100]\tvalid's l1: 0.0523478\n",
      "[1150]\tvalid's l1: 0.0523221\n",
      "[1200]\tvalid's l1: 0.0522989\n",
      "[1250]\tvalid's l1: 0.0522888\n",
      "[1300]\tvalid's l1: 0.052268\n",
      "[1350]\tvalid's l1: 0.052253\n",
      "[1400]\tvalid's l1: 0.0522422\n",
      "[1450]\tvalid's l1: 0.0522141\n",
      "[1500]\tvalid's l1: 0.0522062\n",
      "[1550]\tvalid's l1: 0.0521968\n",
      "[1600]\tvalid's l1: 0.0521906\n",
      "[1650]\tvalid's l1: 0.0521754\n",
      "[1700]\tvalid's l1: 0.0521682\n",
      "[1750]\tvalid's l1: 0.0521629\n",
      "[1800]\tvalid's l1: 0.0521531\n",
      "[1850]\tvalid's l1: 0.0521492\n",
      "[1900]\tvalid's l1: 0.0521397\n",
      "[1950]\tvalid's l1: 0.052134\n",
      "[2000]\tvalid's l1: 0.05213\n",
      "[2050]\tvalid's l1: 0.0521252\n",
      "[2100]\tvalid's l1: 0.0521248\n",
      "[2150]\tvalid's l1: 0.0521237\n",
      "[2200]\tvalid's l1: 0.0521135\n",
      "[2250]\tvalid's l1: 0.0521047\n",
      "[2300]\tvalid's l1: 0.0521036\n",
      "[2350]\tvalid's l1: 0.0521111\n",
      "Early stopping, best iteration is:\n",
      "[2274]\tvalid's l1: 0.0521011\n",
      "stacking的score: 0.07942425\n"
     ]
    }
   ],
   "source": [
    "# 使用全量数据进行模型训练\n",
    "oof_lgb,models=lgb_model(usedata[end_usecol].values,usedata.Qi,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "from matplotlib.font_manager import FontProperties\n",
    "myfont=FontProperties(fname=r'C:\\Windows\\Fonts\\simhei.ttf',size=14)\n",
    "sns.set(font=myfont.get_name())\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feature_imp = pd.DataFrame({'Value':models[0].feature_importance(),'Feature':end_usecol})\n",
    "# plt.figure(figsize=(40, 20))\n",
    "# sns.set(font_scale = 5)\n",
    "# sns.set_style('whitegrid', {'font.sans-serif': ['simhei', 'Arial']})\n",
    "# sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:20])\n",
    "# plt.title('LightGBM Features (avg over folds)')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('lgbm_importances-01.png')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分段一 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果存放到res中，分别对56个值进行预测，上一个时间段的预测，作为下一个时间段的输入，更新前8个时间点特征\n",
    "res = []\n",
    "for i in range(56):\n",
    "    tv = 0\n",
    "    #五折交叉后的模型取平均\n",
    "    for model in models:\n",
    "        pred = model.predict(cut_1[i:1+i][end_usecol].values)[0]\n",
    "        tv+=pred\n",
    "    ev = tv/len(models)\n",
    "    res.append(ev)\n",
    "    cut_1.loc[i+1,'Qi_1'] = ev\n",
    "    cut_1.loc[i+1,'Qi_2'] = cut_1.loc[i,'Qi_1']\n",
    "    cut_1.loc[i+1,'Qi_3'] = cut_1.loc[i,'Qi_2']\n",
    "    cut_1.loc[i+1,'Qi_4'] = cut_1.loc[i,'Qi_3']\n",
    "    cut_1.loc[i+1,'Qi_5'] = cut_1.loc[i,'Qi_4']\n",
    "    cut_1.loc[i+1,'Qi_6'] = cut_1.loc[i,'Qi_5']\n",
    "    cut_1.loc[i+1,'Qi_7'] = cut_1.loc[i,'Qi_6']\n",
    "    cut_1.loc[i+1,'Qi_8'] = cut_1.loc[i,'Qi_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分段2预测，注释思路同分段一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = []\n",
    "for i in range(56):\n",
    "    tv = 0\n",
    "    for model in models:\n",
    "        pred = model.predict(cut_2[i:1+i][end_usecol].values)[0]\n",
    "        tv+=pred\n",
    "    ev = tv/len(models)\n",
    "    res2.append(ev)\n",
    "    cut_2.loc[i+1,'Qi_1'] = ev\n",
    "    cut_2.loc[i+1,'Qi_2'] = cut_2.loc[i,'Qi_1']\n",
    "    cut_2.loc[i+1,'Qi_3'] = cut_2.loc[i,'Qi_2']\n",
    "    cut_2.loc[i+1,'Qi_4'] = cut_2.loc[i,'Qi_3']\n",
    "    cut_2.loc[i+1,'Qi_5'] = cut_2.loc[i,'Qi_4']\n",
    "    cut_2.loc[i+1,'Qi_6'] = cut_2.loc[i,'Qi_5']\n",
    "    cut_2.loc[i+1,'Qi_7'] = cut_2.loc[i,'Qi_6']\n",
    "    cut_2.loc[i+1,'Qi_8'] = cut_2.loc[i,'Qi_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分段3预测，注释思路同分段一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = []\n",
    "for i in range(56):\n",
    "    tv = 0\n",
    "    for model in models:\n",
    "        pred = model.predict(cut_3[i:1+i][end_usecol].values)[0]\n",
    "        tv+=pred\n",
    "    ev = tv/len(models)\n",
    "    res3.append(ev)\n",
    "    cut_3.loc[i+1,'Qi_1'] = ev\n",
    "    cut_3.loc[i+1,'Qi_2'] = cut_3.loc[i,'Qi_1']\n",
    "    cut_3.loc[i+1,'Qi_3'] = cut_3.loc[i,'Qi_2']\n",
    "    cut_3.loc[i+1,'Qi_4'] = cut_3.loc[i,'Qi_3']\n",
    "    cut_3.loc[i+1,'Qi_5'] = cut_3.loc[i,'Qi_4']\n",
    "    cut_3.loc[i+1,'Qi_6'] = cut_3.loc[i,'Qi_5']\n",
    "    cut_3.loc[i+1,'Qi_7'] = cut_3.loc[i,'Qi_6']\n",
    "    cut_3.loc[i+1,'Qi_8'] = cut_3.loc[i,'Qi_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分段4预测，注释思路同分段一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res4 = []\n",
    "for i in range(56):\n",
    "    tv = 0\n",
    "    for model in models:\n",
    "        pred = model.predict(cut_4[i:1+i][end_usecol].values)[0]\n",
    "        tv+=pred\n",
    "    ev = tv/len(models)\n",
    "    res4.append(ev)\n",
    "    cut_4.loc[i+1,'Qi_1'] = ev\n",
    "    cut_4.loc[i+1,'Qi_2'] = cut_4.loc[i,'Qi_1']\n",
    "    cut_4.loc[i+1,'Qi_3'] = cut_4.loc[i,'Qi_2']\n",
    "    cut_4.loc[i+1,'Qi_4'] = cut_4.loc[i,'Qi_3']\n",
    "    cut_4.loc[i+1,'Qi_5'] = cut_4.loc[i,'Qi_4']\n",
    "    cut_4.loc[i+1,'Qi_6'] = cut_4.loc[i,'Qi_5']\n",
    "    cut_4.loc[i+1,'Qi_7'] = cut_4.loc[i,'Qi_6']\n",
    "    cut_4.loc[i+1,'Qi_8'] = cut_4.loc[i,'Qi_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分段5预测，注释思路同分段一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res5 = []\n",
    "for i in range(56):\n",
    "    tv = 0\n",
    "    for model in models:\n",
    "        pred = model.predict(cut_5[i:1+i][end_usecol].values)[0]\n",
    "        tv+=pred\n",
    "    ev = tv/len(models)\n",
    "    res5.append(ev)\n",
    "    cut_5.loc[i+1,'Qi_1'] = ev\n",
    "    cut_5.loc[i+1,'Qi_2'] = cut_5.loc[i,'Qi_1']\n",
    "    cut_5.loc[i+1,'Qi_3'] = cut_5.loc[i,'Qi_2']\n",
    "    cut_5.loc[i+1,'Qi_4'] = cut_5.loc[i,'Qi_3']\n",
    "    cut_5.loc[i+1,'Qi_5'] = cut_5.loc[i,'Qi_4']\n",
    "    cut_5.loc[i+1,'Qi_6'] = cut_5.loc[i,'Qi_5']\n",
    "    cut_5.loc[i+1,'Qi_7'] = cut_5.loc[i,'Qi_6']\n",
    "    cut_5.loc[i+1,'Qi_8'] = cut_5.loc[i,'Qi_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果提交保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.vstack((np.array(res).reshape(-1,),np.array(res2).reshape(-1,),np.array(res3).reshape(-1,),np.array(res4).reshape(-1,),np.array(res5).reshape(-1,)))\n",
    "submit = pd.read_csv('./submission_sample.csv',index_col=0)\n",
    "for i in range(len(yhat)):\n",
    "    submit.iloc[i] = yhat[i]\n",
    "submit.to_csv(\"./submit_result_notebook.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07231856 0.07783443 0.08994661 0.10403395 0.08797615 0.0774916\n",
      "  0.0958493  0.09338269 0.0522257  0.05085063 0.05590374 0.0741131\n",
      "  0.08083397 0.08234105 0.09585103 0.09108399 0.04639896 0.04262129\n",
      "  0.04326766 0.05756588 0.07294454 0.07708597 0.08974921 0.08768414\n",
      "  0.04704357 0.04014701 0.04084164 0.05468963 0.07095588 0.073655\n",
      "  0.0852921  0.08771224 0.04738443 0.03951432 0.04045887 0.05444195\n",
      "  0.07031189 0.07217659 0.08385875 0.08608842 0.04705105 0.04005095\n",
      "  0.04044207 0.05424767 0.07025752 0.07217859 0.08233523 0.08590812\n",
      "  0.04704898 0.04004665 0.04050621 0.05445835 0.07026524 0.07215828\n",
      "  0.08202857 0.08552902]\n",
      " [0.17026502 0.16063447 0.15335194 0.17831549 0.15909378 0.15289691\n",
      "  0.18008282 0.19300454 0.11336044 0.10422385 0.10578583 0.14957715\n",
      "  0.15566803 0.14467102 0.17368673 0.18719243 0.09413693 0.07635691\n",
      "  0.07904201 0.11605363 0.13907717 0.13072418 0.16765797 0.18162082\n",
      "  0.08095356 0.06687074 0.06444722 0.09812918 0.12478219 0.124923\n",
      "  0.14665493 0.16619592 0.07289914 0.06054273 0.05619224 0.08341347\n",
      "  0.10795795 0.10945865 0.13358268 0.15111701 0.06503495 0.05212118\n",
      "  0.053327   0.07548063 0.09650692 0.09740592 0.12194829 0.13746671\n",
      "  0.05915171 0.04948689 0.05140305 0.06946545 0.08388454 0.08737339\n",
      "  0.10938538 0.12469004]\n",
      " [0.07120435 0.07549895 0.07627503 0.07668408 0.08294674 0.07763172\n",
      "  0.09312438 0.12049201 0.06234849 0.05232726 0.05555396 0.07163583\n",
      "  0.08012552 0.08026976 0.09939791 0.11191092 0.05607012 0.04783206\n",
      "  0.04834195 0.06432603 0.07699269 0.08147305 0.0996655  0.10965794\n",
      "  0.05387151 0.04661564 0.0476409  0.06128104 0.07565125 0.07955076\n",
      "  0.09859317 0.10285611 0.05419656 0.04460635 0.0458186  0.05980298\n",
      "  0.07282953 0.07831157 0.09328487 0.09773136 0.04940692 0.04374431\n",
      "  0.04465562 0.05849706 0.07194631 0.07662845 0.0898341  0.0897267\n",
      "  0.04935493 0.04351294 0.04278012 0.05621685 0.0723319  0.07718858\n",
      "  0.08994685 0.08917357]\n",
      " [0.03638273 0.03739383 0.0369608  0.04059968 0.04947108 0.05582954\n",
      "  0.06801604 0.07035713 0.04188596 0.03856618 0.03860801 0.04510996\n",
      "  0.06430584 0.06877222 0.0781193  0.08176254 0.04799279 0.04022146\n",
      "  0.04027894 0.0546186  0.07212531 0.07347832 0.0837939  0.08534746\n",
      "  0.04923753 0.0430241  0.04210036 0.05543381 0.07197805 0.07477198\n",
      "  0.08526715 0.08800747 0.04918931 0.04300675 0.04243723 0.05542127\n",
      "  0.07212634 0.0745874  0.08613304 0.08816593 0.04913612 0.04294953\n",
      "  0.04244966 0.05543128 0.0720079  0.07462049 0.0865921  0.08816593\n",
      "  0.04913945 0.04294953 0.04232313 0.05542743 0.0720079  0.07459624\n",
      "  0.08664572 0.08816593]\n",
      " [0.15848325 0.14355882 0.10011259 0.16640579 0.14779749 0.16076511\n",
      "  0.21638811 0.19007788 0.10526685 0.09716408 0.08050381 0.13531394\n",
      "  0.14370504 0.13730597 0.16866255 0.1809778  0.08958754 0.06970392\n",
      "  0.06786121 0.113418   0.1329359  0.12634233 0.16495478 0.17759814\n",
      "  0.07572691 0.06345756 0.06097226 0.09377146 0.11745165 0.11864787\n",
      "  0.14276382 0.16172928 0.0697084  0.0565749  0.05531539 0.07926702\n",
      "  0.10204252 0.10490462 0.12866403 0.14058854 0.05935888 0.04945765\n",
      "  0.05204151 0.07243955 0.09286052 0.09217155 0.11472979 0.12501249\n",
      "  0.05329246 0.04770918 0.04719359 0.06308782 0.07874271 0.08638681\n",
      "  0.1054737  0.11431056]]\n"
     ]
    }
   ],
   "source": [
    "# 打印结果\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
